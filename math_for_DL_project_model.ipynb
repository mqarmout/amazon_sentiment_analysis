{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from transformers import TFRobertaModel, RobertaTokenizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Data/Reviews.csv')\n",
    "\n",
    "# Map 'Score' to sentiment labels\n",
    "def map_score_to_label(score):\n",
    "    if score in [1, 2]:\n",
    "        return 0  # Negative\n",
    "    elif score == 3:\n",
    "        return 1  # Neutral\n",
    "    else:  # 4 or 5\n",
    "        return 2  # Positive\n",
    "\n",
    "df['label'] = df['Score'].apply(map_score_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset by sampling 5000 reviews per class\n",
    "df_balanced = df.groupby('label').apply(lambda x: x.sample(n=5000, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Clean the text\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove non-ASCII characters\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df_balanced['cleaned_text'] = df_balanced['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "tokenized_texts = tokenize_function(df_balanced['cleaned_text'])\n",
    "\n",
    "# Convert tensors to numpy arrays\n",
    "input_ids = tokenized_texts['input_ids'].numpy()\n",
    "attention_mask = tokenized_texts['attention_mask'].numpy()\n",
    "labels = df_balanced['label'].values\n",
    "\n",
    "# Clear unused variables to save memory\n",
    "del tokenized_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train, validation, and test sets\n",
    "train_input_ids, temp_input_ids, train_labels, temp_labels, train_attention_mask, temp_attention_mask = train_test_split(\n",
    "    input_ids,\n",
    "    labels,\n",
    "    attention_mask,\n",
    "    test_size=0.3,  # 70% train, 30% temp\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Further split temp into validation and test sets\n",
    "val_input_ids, test_input_ids, val_labels, test_labels, val_attention_mask, test_attention_mask = train_test_split(\n",
    "    temp_input_ids,\n",
    "    temp_labels,\n",
    "    temp_attention_mask,\n",
    "    test_size=0.5,  # 15% validation, 15% test\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Clear unused variables to save memory\n",
    "del input_ids, attention_mask, labels, temp_input_ids, temp_labels, temp_attention_mask\n",
    "\n",
    "# Function to create TensorFlow datasets\n",
    "def create_tf_dataset(input_ids, attention_mask, labels, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {'input_ids': input_ids, 'attention_mask': attention_mask},\n",
    "        labels\n",
    "    ))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model function\n",
    "def build_model(base_model, activation_function='relu', dropout_rate=0.2, use_batchnorm=False):\n",
    "    # Define inputs\n",
    "    input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    # Get outputs from the base model\n",
    "    outputs = base_model(input_ids, attention_mask=attention_mask)\n",
    "    cls_token = outputs.last_hidden_state[:, 0, :]\n",
    "    \n",
    "    # Add new encoding layers\n",
    "    x = tf.keras.layers.Dense(512)(cls_token)\n",
    "    if use_batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation_function)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(256)(x)\n",
    "    if use_batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation_function)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128)(x)\n",
    "    if use_batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation_function)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(64)(x)\n",
    "    if use_batchnorm:\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation(activation_function)(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    # Build the model\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get optimizer\n",
    "def get_optimizer(name, learning_rate=5e-5):\n",
    "    if name == 'adam':\n",
    "        return tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif name == 'sgd':\n",
    "        return tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f'Unsupported optimizer: {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations\n",
    "configurations = [\n",
    "    {'optimizer': 'adam', 'batch_size': 32, 'dropout_rate': 0.1, 'activation_function': 'relu', 'use_batchnorm': False},\n",
    "    {'optimizer': 'adam', 'batch_size': 32, 'dropout_rate': 0.2, 'activation_function': 'tanh', 'use_batchnorm': True},\n",
    "    {'optimizer': 'sgd', 'batch_size': 64, 'dropout_rate': 0.1, 'activation_function': 'relu', 'use_batchnorm': False},\n",
    "    {'optimizer': 'sgd', 'batch_size': 16, 'dropout_rate': 0.2, 'activation_function': 'tanh', 'use_batchnorm': True},\n",
    "]\n",
    "\n",
    "# Load the base RoBERTa model and freeze layers\n",
    "base_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "histories = []\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over configurations\n",
    "for i, config in enumerate(configurations):\n",
    "    print(f\"Training configuration {i+1}/{len(configurations)}: {config}\")\n",
    "    \n",
    "    # Build the model\n",
    "    model = build_model(\n",
    "        base_model,\n",
    "        activation_function=config['activation_function'],\n",
    "        dropout_rate=config['dropout_rate'],\n",
    "        use_batchnorm=config['use_batchnorm']\n",
    "    )\n",
    "    \n",
    "    # Get optimizer\n",
    "    optimizer = get_optimizer(config['optimizer'])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create datasets with specified batch size\n",
    "    batch_size = config['batch_size']\n",
    "    train_dataset = create_tf_dataset(train_input_ids, train_attention_mask, train_labels, batch_size)\n",
    "    val_dataset = create_tf_dataset(val_input_ids, val_attention_mask, val_labels, batch_size)\n",
    "    test_dataset = create_tf_dataset(test_input_ids, test_attention_mask, test_labels, batch_size)\n",
    "    \n",
    "    # Set up callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    \n",
    "    # Fit the model using the validation set\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=5,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Record the history and config\n",
    "    histories.append((history, config))\n",
    "    \n",
    "    # Evaluate the final model on the test set\n",
    "    loss, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "    \n",
    "    # Generate predictions for the test set\n",
    "    y_pred_probs = model.predict(test_dataset)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    \n",
    "    # Flatten test labels (in case they are in batches)\n",
    "    y_true = test_labels\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_true, y_pred, target_names=['Negative', 'Neutral', 'Positive'])\n",
    "    print(f\"Classification Report for Configuration {i+1}:\\n{report}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.title(f'Confusion Matrix for Configuration {i+1}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Store results\n",
    "    results.append({'config': config, 'accuracy': accuracy, 'loss': loss, 'confusion_matrix': cm, 'classification_report': report})\n",
    "    \n",
    "    # Clear session and delete model to free memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "\n",
    "# Plot training and validation accuracy for each configuration\n",
    "for i, (history, config) in enumerate(histories):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f\"Configuration {i+1}: {config}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss for each configuration\n",
    "for i, (history, config) in enumerate(histories):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f\"Configuration {i+1}: {config}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of the results\n",
    "results_df = pd.DataFrame([{\n",
    "    'optimizer': res['config']['optimizer'],\n",
    "    'batch_size': res['config']['batch_size'],\n",
    "    'dropout_rate': res['config']['dropout_rate'],\n",
    "    'activation_function': res['config']['activation_function'],\n",
    "    'use_batchnorm': res['config']['use_batchnorm'],\n",
    "    'accuracy': res['accuracy'],\n",
    "    'loss': res['loss']\n",
    "} for res in results])\n",
    "\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
